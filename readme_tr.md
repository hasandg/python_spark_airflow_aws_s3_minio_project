# Docker, Spark, Airflow, ve MinIO Kullanarak Veri Analizi


Bu proje, veri analizi iÅŸ akÄ±ÅŸÄ±nÄ± oluÅŸturmak iÃ§in Docker, Spark, Airflow ve MinIO teknolojilerini kullanarak gÃ¼Ã§lÃ¼ ve
esnek bir Ã§Ã¶zÃ¼m sunuyor. Projenin amacÄ±, veri setini oluÅŸturan iki adet CSV dosyasÄ±nÄ± MinIO'dan yÃ¼klemek, Spark'Ä± 
kullanarak veri setini analiz etmek ve sonuÃ§larÄ± yine MinIO'ya kaydetmektir.

![diagram](./docker_spark_airflow_minio.png)

## ğŸ›  KullanÄ±lan Teknolojiler ve Metodoloji


1. PySpark kullanÄ±larak Python dilinde bir script yazÄ±ldÄ±. Bu script, Airflow scheduler desteÄŸi ile 10 dakikada bir verileri MinIO'dan okuyup, Apache Spark kullanarak veri analizi yapar ve sonuÃ§larÄ± MinIO'ya kaydeder.
2. Docker ile apache/airflow:2.9.3 base imajÄ± kullanÄ±larak gÃ¼ncel Java ve PySpark kÃ¼tÃ¼phaneleri iÃ§eren Ã¶zel bir Docker imajÄ± oluÅŸturuldu. MinIO eriÅŸimi iÃ§in; AWS'in disk Ã¼rÃ¼nÃ¼ olan S3 ile uyumlu baÄŸlantÄ± kÃ¼tÃ¼phanesini iÃ§eren jar dosyalarÄ± da bu imaja eklendi.

3. Proje, aÅŸaÄŸÄ±daki sÃ¼rÃ¼mlerle geliÅŸtirilmiÅŸ ve test edilmiÅŸtir:


| YazÄ±lÄ±m        | Versiyon     |
|----------------|--------------|
| OpenJDK        | `17.0.2`     |
| Docker         | `25.0.3`     |
| Docker Compose | `2.24.6`     |
| Spark          | `3.5.1`      |
| Hadoop         | `3`          |
| PySpark        | `3.5.1`      |
| Python         | `3.12.4`     |
| Airflow        | `2.9.3`      |
| MinIO          | `2024.7.16`  |


# ğŸš€ UygulamanÄ±n Ã‡alÄ±ÅŸtÄ±rÄ±lmasÄ±
## 1. Ã–zel Airflow Ä°majÄ±nÄ±n OluÅŸturulmasÄ±

    $ cd python_spark_airflow_aws_s3_minio/src/
    $ docker build --rm -t docker-airflow-custom1:latest .  

## 2. Docker Compose ile UygulamanÄ±n Ã‡alÄ±ÅŸtÄ±rÄ±lmasÄ±
Container'larÄ± baÅŸlatmak iÃ§in:

    $ cd python_spark_airflow_aws_s3_minio/
    $ docker-compose up -d

Ã‡alÄ±ÅŸma sonrasÄ±nda container'larÄ± durdurmak iÃ§in:

    $ docker-compose down


## MinIO Webserver'a verilerin yÃ¼klenmesi

[http://localhost:9001](http://localhost:9001)

MinIO webserver'a eriÅŸmek iÃ§in kullanÄ±cÄ± adÄ± ve ÅŸifre: 

``` User: minioadmin``` <br> ``` Pass: minioadmin```

MinIO webserver'a eriÅŸildikten sonra, `data` adÄ±nda bir bucket oluÅŸturulmalÄ±dÄ±r.
"person_data.csv" ve "country_data.csv" dosyalarÄ± bu bucket iÃ§ine yÃ¼klenmelidir.

## Airflow Webserver'a eriÅŸim

[http://localhost:8060](http://localhost:8060)

Airflow webserver'a eriÅŸmek iÃ§in kullanÄ±cÄ± adÄ± ve ÅŸifre: 

``` User: airflow``` <br> ``` Pass: airflow```

python_spark_airflow_aws_s3_minio/dags klasÃ¶rÃ¼ne eklenen dag dosyalarÄ± otomatik olarak airflow webserver'da gÃ¶rÃ¼ntÃ¼lenecektir.
Bu dosyalar web arayÃ¼zÃ¼nden etkinleÅŸtirilerek Ã§alÄ±ÅŸtÄ±rÄ±labilir. Bu arayÃ¼zden "data_processing_dag.py" dosyasÄ±nÄ± etkinleÅŸtirerek 
sonuÃ§larÄ± MinIO webserver'da bulunan data dizininden kontrol edebilirsiniz.

`python_spark_airflow_aws_s3_minio/dags` klasÃ¶rÃ¼ne eklenen DAG dosyalarÄ± otomatik olarak Airflow webserver'da gÃ¶rÃ¼ntÃ¼lenecektir. 
Bu dosyalar web arayÃ¼zÃ¼nden etkinleÅŸtirilerek Ã§alÄ±ÅŸtÄ±rÄ±labilir. Bu arayÃ¼zden `data_processing_dag.py` dosyasÄ±nÄ± etkinleÅŸtirerek 
sonuÃ§larÄ± MinIO webserver'da bulunan `data` dizininden kontrol edebilirsiniz.

## EriÅŸim Bilgileri 

| Uygulama       | URL                                            | KullanÄ±cÄ± adÄ± ve ÅŸifre                               |
|----------------|------------------------------------------------|------------------------------------------------------|
| Airflow        | [http://localhost:8060](http://localhost:8085) | ``` User: airflow``` <br> ``` Pass: airflow```       |         |
| MinIO          | [http://localhost:9001](http://localhost:9001) | ``` User: minioadmin``` <br> ``` Pass: minioadmin``` |           |
| Postgres       | **Server/Database:** localhost:5432/airflow    | ``` User: airflow``` <br> ``` Pass: airflow```       |           | 